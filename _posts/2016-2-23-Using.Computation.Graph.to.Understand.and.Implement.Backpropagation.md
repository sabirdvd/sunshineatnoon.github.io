---
layout: post
title: Using Computation Graph to Understand and Implement Backpropagation
published: true
use_math: true
---

Recently, by hacking on Assignment2 of the awsome openclass [cs231n](http://vision.stanford.edu/teaching/cs231n/syllabus.html), I finally stop bumping my head to the wall every time I came across backpropagation, now I even feel a little bit enjoy when hacking the backpropagation process! 
In this post, I will show how to use computation graph to implement  both forward and backward process of Batch Normailzation, Convolution and Pooling. 

## Batch Normailzation

[Bacth Normailzation](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf) is a recently proposed method to alleviate the pain in training neural network, especially the special care one needs when initializing weights. Here I won't go into details to explain the method itself, the details can be found from the [paper](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf). But for a quick summary, batch normalization does two things:

- make activation of fully connected or convolutional layer unit gaussian by normalizing the activation
- scale and shift normalized activation so that the network can "choose" to cancel the above step

The algorithm can be summarized like this:

![enter image description here](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/batch_normalization.png)

#### Computation Graph
Building a computation graph is easy, the key is to split operations into basic operations such as add, multiplication or sqrt, but keep in mind don't split operations too broke, the larger the computation graph, the messier you will feel when backpropagate.
The computation graph of Batch Normalization is like this:
![enter image description here](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/cg_batch_norm.png)
The black lines and equations are for the forward pass while the red ones are for the backward pass. Since each gate only completes a simple operation, both the forward pass and backward pass is quite straightfoward. However, special attention might be paid to the summation gate, it confused me for a while. We can see this gate from equation prospect and matrix(coding) prospect:

![enter image description here](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/sum_back.png)

You see, in the forward pass, you sum each row element-wisely to a single row, so each element "contributes" equally to the error. Thus, in the backpropagation pass, you only need to distribute the gradients evenly to rows like the red parts in the graph. 
Another prospect is to regard this summation as a simple addition like $$out = row_1+row_2+...+row_m$$, so each $row_i$ gets 1 as its local gradient, and you only need to multiply this local gradient with the gradients received by this gate to get $row_i's$  true gradient.
#### Feedforward Pass Code

With the computation graph above, the code is quite easy to write:

```
batch_mean = np.sum(x,axis=0,keepdims=True) / N
batch_var = np.sum((x - batch_mean) ** 2,axis=0,keepdims=True) / N

x_minus_mu = x - batch_mean
ivar = 1.0 / np.sqrt(batch_var+eps)
x_hat = x_minus_mu * ivar
out = gamma * x_hat + beta
running_mean = momentum * running_mean + (1 - momentum) * batch_mean
running_var = momentum * running_var + (1 - momentum) * batch_var
cache = (gamma,x,batch_var,batch_mean,eps,x_minus_mu,ivar,x_hat)
```
 Above is the train forward pass, the test forward pass is easier since your only need to normalize by running_mean and running_var accumulated during training. To avoid messy details about Batch Normalization, I leave that part for now.

#### Backward Pass Code
With the computation graph, the backward pass falls into a step-by-step implementation of equations in the computation graph. A trick to be used is that when you don't know the order of multiplication, use dimension deduction. For instance, when implementing $$d\hat{x} = d\hat{x}\gamma * d\gamma$$ I know that the dimensions of $d\hat{x}, d\hat{x}\gamma$ and $d\gamma $ are (N,D), (N,D) and (D), so the code can be written as:
```
dx_hat = dout * gamma
```
The full code for backpropagation is like this:
```
gamma,x,batch_var,batch_mean,eps,x_minus_mu,ivar,x_hat = cache
N, D = x.shape

dgamma = np.sum(x_hat * dout,axis=0)
dbeta = np.sum(dout,axis=0)
dx_hat = dout * gamma

dinv_sqrt_var = np.sum(dx_hat * x_minus_mu,axis=0)
dvar = -0.5*((batch_var+eps)**(-1.5)) * dinv_sqrt_var
dx_minus_mu_sqrt = (1.0 / N) * np.ones((N,D)) * dvar
dx_minus_mu = 2 * x_minus_mu * dx_minus_mu_sqrt + dx_hat * ivar
dmu = -np.sum(dx_minus_mu,axis=0)
dx = dx_minus_mu + (1.0 / N) * np.ones((N,D))*dmu
```
Each line is just an equation in the computation graph.

To be continued....
## Reference

[1] [http://cs231n.github.io/neural-networks-2/](http://cs231n.github.io/neural-networks-2/)

[2] [http://vision.stanford.edu/teaching/cs231n/slides/winter1516_lecture5.pdf](http://vision.stanford.edu/teaching/cs231n/slides/winter1516_lecture5.pdf)

[3] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.





















