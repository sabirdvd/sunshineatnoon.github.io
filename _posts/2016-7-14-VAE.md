---
layout: post
title: Notes on Auto-Encoding Variational Bayes
published: true
use_math: true
---

### Purpose of Variational Inference:

Suppose:

- Observable Data: $X = {x_1,x_2,...,x_m}$
- Hidden Variable: $Z = {z_1,z_2,...,z_n}$
- Known their joint distribution: $p(x,z)$

Then Variational Inference is used to find a distribution $q(z)$ to approximate conditional distribution $p(z\|x)$

### KL Divergence
-  A way to measure closeness bewteen $q(z)$ and $p(z\|x)$
- Definition: 
$$D_{KL}[q(z)||p(z\|x)] = \int q(z)log \frac{q(z)}{p(z\|x)}dz$$

Then our goal is to minimize $D_{KL}[q(z)||p(z\|x)]$. By a series of mathmatical manipulation, we have:
![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/KL.png)
The negative of the first term on the right hand side in the above equation is called "**Evidence Lower Bound(ELB)**". Since we want to measure the probability of z **given x**, thus $\log{p(x)}$ is fixed, so 

> Minimizing $D_{KL}[q(z)\|\|p(z\|x)] \Longleftrightarrow$ Maximizing ELB.

### Evidence Lower Bound
![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/ELB.png)

> Maximizing ELB $\Longleftrightarrow$ Minimizing $D_{KL}[q_\phi (z|x)\|p_\theta(x\|z)]$ and Maximizing $E_{q_{\phi}(z|x)}[logp_\theta(x|z)]$.

### Reparameterization

- What is reparameterization ?

Reparameterization is a way to express the random variable z as a deterministic variable $z = g_\phi(\varepsilon ,x)$, where $\varepsilon$ is an auxiliary variable with independent marginal $p(\varepsilon)$, and $g_\phi(.)$ is some vector-valued function parameterized by $\phi$.

To put it simple, reparameterization is a new way to sample z, instead of sample z directly from $p(z)$, we first sample an $\varepsilon$ from $p(\varepsilon)$ , and then use function $z = g_\phi(\varepsilon ,x)$ to get our z.

- Why do we need reparameterization ?

Because we don't have "sampling layers" in neural networks.  Say our encoder gives us a distribution $q(z|x)$ to approximate conditional distribution $p(z\|x)$, how can we sample a latent variable z from $p(z\|x)$? We can't. But with reparameterization, we can first sample an $\varepsilon$ from $p(\varepsilon)$, then let our encoder gives us $g_\phi(.)$, then we can compute $z$ by $z = g_\phi(\varepsilon ,x)$.

- An example of reparameterization
Take, for example, the univariate Gaussian case: if $$z ∼ p(z|x) = N(
\mu,\sigma^2)$$ In this case, a valid reparameterization is $$z = \mu + \varepsilon\sigma$$
where $\varepsilon$ is an auxiliary noise variable $\epsilon ∼ N (0, 1)$. Therefore,
$$E_{N(z;\mu,\sigma^2)}[f(z)] = E_{N(\varepsilon;0,1)}[f(\mu+\sigma\varepsilon)] \simeq {\frac{1}{L}} \sum\limits_{l=1}^{L}f(\mu+\sigma \varepsilon(l))$$
where $ \varepsilon(l) \sim N(0,1)$

### VAE (Variational Auton Encoder)
For VAEs, we make several assumptions based on above variational inference:

- Assume the prior over the latent variables $p_{\theta(z)} = N(z;0,I) $
- Assume the true posterior $p(z|x) \sim N(\mu,\sigma)$ 
- Let $p_\theta(x|z)$ be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data), then we will have $$log q_\phi(z|x^{(i)}) = log N (z; \mu(i), \sigma^2(i)I)$$

Our auto encoder will look like this:

![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/VAE.png)
  
Our algorithm goes like this:

 - we feed our training sample $x^{i}$ into our encoder, then our encoder gives the parameters (i.e. $\mu$ and $\sigma$) of distribution $p(z|x) \sim N(\mu,\sigma)$
 - By the reparameterization trick and ($\mu$, $\sigma$), we compute $z$ by $z = \mu + \varepsilon\sigma$
 - We feed the latent variable $z$ into our decoder and get the reconstruction of the training sample $x$

Loss function:

> Maximizing ELB $\Longleftrightarrow$ Minimizing $D_{KL}[q_\phi (z|x)\|p_\theta(x\|z)]$ and Maximizing $E_{q_{\phi}(z|x)}[logp_\theta(x|z)]$.

Thus the loss function consists of two parts: the KLD and $E_{q_{\phi}(z|x)}[logp_\theta(x|z)]$. 

For the KLD loss, since both $q_\phi (z|x)$ and $p_\theta(x\|z)$ are Guassian distributions, we can compute and differentiate the KLD without estimation. In appendix B, the paper[4] gives us the KLD solution for $q_\phi (z|x)$ being Gaussian:

![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/KLD_Gaussian.png)

For the decoding loss term $E_{q_{\phi}(z|x)}[logp_\theta(x|z)]$, it's either Bernoulli or multivariate Gaussian.  If your data is binary, you can use [Binary Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy) for $E_{q_{\phi}(z|x)}[logp_\theta(x|z)]$. Or Gaussian Loss if your data is continous.

###Code
There's many implementations of VAE, I recommend this [one](https://github.com/y0ast/VAE-Torch) in torch, the code is very written and easy to understand.

Reference:

[1] [http://blog.themusio.com/2016/06/30/variational-autoencoder/](http://blog.themusio.com/2016/06/30/variational-autoencoder/)

[2] [Tutorial on Variational Autoencoders](http://arxiv.org/pdf/1606.05908v1.pdf)

[3] [Variational Inference Video Tutorial](https://www.youtube.com/playlist?list=PLdk2fd27CQzSd1sQ3kBYL4vtv6GjXvPsE)

[4] Rezende D J, Mohamed S. Variational inference with normalizing flows[J]. arXiv preprint arXiv:1505.05770, 2015.

[5] [VAE in Torch](https://github.com/y0ast/VAE-Torch)
