{\rtf1\ansi\ansicpg936\cocoartf1404\cocoasubrtf110
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf0 \expnd0\expndtw0\kerning0
---\
layout: post\
title: Notes on Auto-Encoding Variational Bayes\
published: true\
---\
\
####Purpose of Variational Inference:\
\
Suppose:\
\
- Observable Data: $X = \{x_1,x_2,...,x_m\}$\
- Hidden Variable: $Z = \{z_1,z_2,...,z_n\}$\
- Known their joint distribution: $p(x,z)$\
\
Then Variational Inference is used to find a distribution $q(z)$ to approximate conditional distribution $p(z|x)$\
\
#### KL Divergence\
-  A way to measure closeness bewteen $q(z)$ and $p(z|x)$\
- Definition: $$D_\{KL\}[q(z)||p(z|x)] = \\int q(z)log \\frac\{q(z)\}\{p(z|x)\}dz$$\
\
Then our goal is to minimize $D_\{KL\}[q(z)||p(z|x)]$. By a series of mathmatical manipulation, we have:\
![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/KL.png)\
The negative of the first term on the right hand side in the above equation is called "**Evidence Lower Bound(ELB)**". Since we want to measure the probability of z **given x**, thus $\\log\{p(x)\}$ is fixed, so \
\
> Minimizing $D_\{KL\}[q(z)||p(z|x)] \\Longleftrightarrow$ Maximizing ELB.\
\
#### Evidence Lower Bound\
![](https://raw.githubusercontent.com/sunshineatnoon/sunshineatnoon.github.io/master/images/ELB.png)\
\
\
\
  \
\
Reference:\
\
[1] [http://blog.themusio.com/2016/06/30/variational-autoencoder/](http://blog.themusio.com/2016/06/30/variational-autoencoder/)\
\
[2] [Tutorial on Variational Autoencoders](http://arxiv.org/pdf/1606.05908v1.pdf)\
\
[3] [Variational Inference Video Tutorial](https://www.youtube.com/playlist?list=PLdk2fd27CQzSd1sQ3kBYL4vtv6GjXvPsE)}